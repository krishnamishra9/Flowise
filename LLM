import paramiko
import os
import sys
from google.cloud import storage

# --- SFTP CONFIG ---
SFTP_HOST = "your.sftp.server"
SFTP_PORT = 22
SFTP_USER = "your_username"
SFTP_PASS = "your_password"
SFTP_BASE_DIR = "MFDESK/Mfdex"   # base path on SFTP

# --- GCS CONFIG ---
GCS_BUCKET = "your-gcs-bucket-name"

# --- LOCAL TEMP DIR ---
LOCAL_DIR = "/tmp/sftp_files"

def download_from_sftp(date_folder, period_type, target_folder=None):
    """
    Download files from SFTP for given date and period_type (weekly/monthly).
    If target_folder (InvestorSeg/DistributorCat/DataFeed) is specified, only that is fetched.
    """
    if not os.path.exists(LOCAL_DIR):
        os.makedirs(LOCAL_DIR)

    transport = paramiko.Transport((SFTP_HOST, SFTP_PORT))
    transport.connect(username=SFTP_USER, password=SFTP_PASS)
    sftp = paramiko.SFTPClient.from_transport(transport)

    # Iterate through folders 2,3,4
    for folder_num in ["2", "3", "4"]:
        base_path = f"{SFTP_BASE_DIR}/{folder_num}/{period_type.capitalize()}/{date_folder}"

        # Determine which subfolders to fetch
        subfolders = [target_folder] if target_folder else ["InvestorSeg", "DistributorCat", "DataFeed"]

        for sub in subfolders:
            remote_path = f"{base_path}/{sub}"
            local_path = os.path.join(LOCAL_DIR, folder_num, period_type, date_folder, sub)
            os.makedirs(local_path, exist_ok=True)

            try:
                file_list = sftp.listdir(remote_path)
            except FileNotFoundError:
                print(f"⚠️ Skipping: {remote_path} not found")
                continue

            for filename in file_list:
                remote_file = f"{remote_path}/{filename}"
                local_file = os.path.join(local_path, filename)
                print(f"⬇️ Downloading {remote_file} → {local_file}")
                sftp.get(remote_file, local_file)

    sftp.close()
    transport.close()


def upload_to_gcs(date_folder, period_type, target_folder=None):
    """Upload downloaded files to GCS using ADC"""
    client = storage.Client()
    bucket = client.bucket(GCS_BUCKET)

    base_local_path = os.path.join(LOCAL_DIR)

    for root, _, files in os.walk(base_local_path):
        for filename in files:
            local_file = os.path.join(root, filename)

            # GCS path: preserve folder structure relative to LOCAL_DIR
            relative_path = os.path.relpath(local_file, LOCAL_DIR)
            gcs_path = f"{relative_path}"

            blob = bucket.blob(gcs_path)
            print(f"⬆️ Uploading {local_file} → gs://{GCS_BUCKET}/{gcs_path}")
            blob.upload_from_filename(local_file)


if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python sftp_to_gcs.py <date_folder: YYYYMMDD> <period_type: weekly|monthly> [target_folder]")
        sys.exit(1)

    date_folder = sys.argv[1]         # e.g. 20250818
    period_type = sys.argv[2].lower() # weekly or monthly
    target_folder = sys.argv[3] if len(sys.argv) > 3 else None

    download_from_sftp(date_folder, period_type, target_folder)
    upload_to_gcs(date_folder, period_type, target_folder)
